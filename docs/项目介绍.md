# AI 长篇小说写作助手 - 项目介绍

## 一、项目背景与功能介绍

### 1.1 项目背景

传统网文/长篇小说创作存在几个典型痛点：
- 设定体量巨大（世界观、人物、时间线），作者很难在几十万甚至上百万字中保持前后一致；
- 单次使用 LLM 写长文时容易“遗忘设定、逻辑崩坏”，越写越偏离原本的世界观和人设；
- 创作初期「从零开始搭世界」门槛高，创作中后期「改设定」又会牵扯大量连锁影响；
- 作者很难获得一个同时懂“网文叙事”和“工程实现”的 AI 搭档。

本项目希望解决的，就是**如何用 LLM + Agent + RAG 构建一套真正能陪作者写完一本长篇的写作工作台**，而不是一个只能生成单次爽文片段的玩具。

### 1.2 项目定位

本项目是一个**面向长篇网文创作的 AI 写作助手**，通过前后端分离架构，将「创意孵化 → 设定构建 → 大纲规划 → 章节写作 → 设定维护」串成一条连续的创作流水线。特点包括：

- 以**项目（Project）**为单位管理整本书的设定与文稿；
- 通过向导式流程（Genesis Wizard）帮助作者从零搭建世界观、人物和主线结构；
- 在写作过程中，利用 RAG 和版本化设定系统，确保人物、世界观、时间线的一致性；
- 支持作者在任意阶段插入新灵感，系统自动协调设定演化和后续章节。

### 1.3 核心功能概览

综合 `docs/需求说明书.md`、`docs/RAG策略.md` 等文档，系统业务功能可大致分为三大模块：

1. **创作引导域（前期）**
   - 创意孵化：基于 LLM + Web 搜索，帮助用户从模糊灵感提炼出书名、卖点和一句话梗概；
   - 世界观搭建：力量体系、势力地图、经济系统等结构化设定；
   - 角色工厂：为主角/配角/反派生成多维度人物卡；
   - 结构编排：从主线结构到分卷大纲，再到前若干章细纲。

2. **正文写作域（中期）**
   - 章节生成：基于当前章节大纲 + RAG 检索到的设定与前文摘要进行写作；
   - 动态干预：用户临时改变情节或设定时，由 Agent 负责调整相关大纲与设定版本；
   - 写作工具箱：命名、环境描写、文风润色等一键辅助工具；
   - 一致性检查：对照设定库自动检查“吃书”与人设崩坏。

3. **设定管理域（全周期）**
   - 动态百科（Wiki）：集中展示人物、地点、势力、道具等实体以及随章节演化的版本；
   - 时间轴视图：以章节为时间线回溯任一时刻的世界状态；
   - 进度看板：展示项目整体进度、已写章节、设定完善度等。

前端上，这些能力主要通过 `GenesisWizard.tsx`、`Studio.tsx`、`Wiki.tsx` 与 `ProjectDashboard.tsx` 等页面完成整体串联。

---

## 二、技术实现与架构设计

> 本节重点展示技术选型与搭配思路：不仅说明“用了什么”，还说明“为什么用、用来解决什么问题”。

### 2.1 整体架构概览

项目采用**前后端分离 + 后端集中编排多 Agent + 外接向量数据库的 RAG 架构**：

- 前端：React + Vite + TypeScript 负责交互与写作体验；
- 后端：FastAPI 作为 HTTP API 层与业务编排核心；
- Agent 编排：使用 LangGraph 构建有状态、多节点的工作流，部分节点内部使用 LangChain 工具组件；
- 知识存储：PostgreSQL + pgvector 存储结构化设定与向量化记忆；
- 检索增强生成（RAG）：结合章节维度与实体维度的检索策略，从知识库中提取“当前必须记住的设定”；
- LLM：通过 OpenAI 兼容接口调用 GPT-4o-mini 等模型，支撑文本生成与结构化提取；
- 搜索：通过 MCP（Model Context Protocol）服务封装 Tavily 等 Web 搜索 API，用于创意分析与补充现实知识；
- MCP 服务：使用标准化的 MCP 协议将外部工具（如搜索、数据源）模块化，便于扩展与替换。

这一套组合的核心目标，是在保证工程可维护性的前提下，实现：

- **多步、可控的 Agent 工作流**（而不是单轮大 Prompt）；
- **项目级、可演化的长期记忆**（而不是一次性上下文堆叠）；
- **前端可观测的生成过程**（流式返回状态与结果）。

### 2.2 前端技术栈与设计（React + Vite）

> 注：原始技术规划文档中建议使用 Next.js，但实际实现中前端位于 `front-end/`，基于 React + Vite 搭建。这里重点说明实际实现与设计考量。

- **框架与构建工具**：
  - 使用 **React + Vite + TypeScript** 实现前端 SPA：
    - React 负责组件化 UI 与状态管理；
    - Vite 提供快速的本地开发与构建体验，适合纯前后端分离架构；
    - TypeScript 使复杂状态（项目、章节、设定）的类型约束更加清晰，减少前端逻辑错误。

- **核心页面组件**（位于 `front-end/components/`）：
  - `GenesisWizard.tsx`：创世向导页面，将创意孵化、世界观、人物设定和大纲生成串成一个多步流程，并实时在右侧展示生成的设定草稿；
  - `Studio.tsx`：写作工作台，承载章节编辑、AI 草稿生成与局部润色；
  - `Wiki.tsx`：项目百科，基于后端的实体/章节信息展示“动态设定”；
  - `ProjectDashboard.tsx` / `ProjectHub.tsx`：项目列表和总览，方便在不同作品间切换。

- **与后端的协作方式**：
  - `front-end/api.ts` 封装了与 FastAPI 后端交互的函数（如项目 CRUD、章节生成、设定加载等）；
  - 对于需要流式反馈的生成任务（如 Genesis 多 Agent 写作），使用浏览器原生流式响应或类似 EventSource/ReadableStream 方案及时更新 UI；
  - 在组件级别，将“业务状态”与“LLM/Agent 状态”拆开管理，例如：
    - `GenesisWizard` 维护一个“步骤状态 + 已生成模块状态”（骨架、主角、世界观、大纲等）；
    - 前端根据后端返回的事件类型（status/result）逐步更新进度条与对应板块。

**为什么不用 Next.js，而选 Vite + 纯前后端分离？**
- 项目更偏“工具型应用”，前端 SSR/SEO 需求不强，保持简单的 SPA 更利于快速迭代；
- 后端已经采用 FastAPI 作为统一 API 层，Next.js 的 API 路由价值降低；
- Vite 的冷启动和热更新体验非常适合偏工具化、交互密集的编辑器项目。

### 2.3 后端：FastAPI + 多 Agent 编排（LangGraph）

#### 2.3.1 FastAPI 作为统一 API 层

后端目录位于 `back-end/app/`，其中：

- `app/main.py`：FastAPI 入口，挂载各业务路由；
- `app/routers/`：按领域拆分的路由模块：
  - `genesis.py`：创世向导相关接口，负责触发多 Agent 工作流并以流式形式返回中间状态与结果；
  - `chat.py`、`chapters.py`、`projects.py`、`entities.py`、`inspiration.py` 等：分别负责写作对话、章节管理、项目管理、实体管理和灵感收集等。

**选用 FastAPI 的原因：**
- 对异步和高并发支持良好，便于与 LLM/向量数据库交互；
- 内置 Pydantic 校验请求与响应数据结构，便于维护复杂的 JSON 结构（如人物卡、世界观设定）；
- 自带 Swagger 文档（`/docs`），便于自测与未来扩展其他客户端。

#### 2.3.2 LangGraph 多 Agent 工作流

后端 Agent 相关代码主要位于 `app/agents/`：

- `state.py`：定义 Agent 流转的全局状态结构，如 `GenesisState`，包含：
  - `skeleton`, `characters`, `world`, `outline`, `first_chapter` 等字段；
- `nodes.py`：实现各类节点（Node）：
  - Router 节点：根据用户输入与当前状态决定下一步调用哪个专长 Agent；
  - 专家 Agent 节点：
    - Skeleton Agent：负责编排故事骨架；
    - Protagonist Agent：生成主角设定；
    - World Agent：生成世界观设定；
    - Outline Agent：生成大纲；
    - First Chapter Agent：生成第一章草稿；
  - Finalizer 节点：聚合各模块结果，返回统一结构给前端。
- `graph.py`：利用 LangGraph 构建有向图，将这些节点串联为：
  - `router → skeleton → protagonist → first_chapter → world → outline → finalizer`

**为什么选 LangGraph 而不是单纯使用 LangChain Chains？**
- 创世流程不是简单的线性 Chain，而是带有**状态、条件分支和循环**的有向工作流；
- LangGraph 提供了**显式的 State + Node + Edge** 模型，使得每一步的输入输出和依赖关系更清晰；
- 在项目中，可以通过 LangGraph 内的状态控制：
  - 已完成模块不重复计算；
  - 能根据用户的中途修改，重新局部生成某些模块；
  - 为未来增加“自动反思/重写”节点提供扩展性。

在对外暴露层面，`app/routers/genesis.py` 使用 FastAPI 的 `StreamingResponse` 将 LangGraph 中每个节点的中间状态以事件流形式发送给前端，实现“创世过程可视化”。

### 2.4 RAG 与项目级记忆：PostgreSQL + pgvector

#### 2.4.1 关系型数据库：结构化“真相”

后端数据库设计主要参考 `docs/RAG策略.md`，实际建表位于 `back-end/schema.sql` 与 `app/db/models.py`。核心思路是：

- **关系库存“真相”**：
  - `Project` 表：存储项目（小说）的基本信息及当前进度；
  - `Chapter` 表：存储章节内容与摘要；
  - `Character` / `World` / `Entity` 等：存储人物、世界观和其他设定实体；
  - 实体版本表（或等效结构）：通过 `chapter_from` / `chapter_to` / `version` 等字段记录设定随章节演化的变化。

这样设计的原因：
- 小说设定具有**时间维度与版本维度**，需要精确表达“第 N 章之前/之后，某个角色或设定处于什么状态”；
- 关系型数据库适合做约束（唯一键、外键）、事务与查询优化，是“权威真相源（source of truth）”。

#### 2.4.2 向量数据库：高效语义检索

在关系库之上，项目使用 **pgvector** 扩展，使 PostgreSQL 同时兼具向量库能力：

- 将人物卡、世界观条目、时间线摘要、既有章节摘要等内容压缩成文本 summary，并计算 embedding；
- 将 embedding 与 `project_id`、`entity_type`、`entity_id`、`version`、`chapter_from`、`chapter_to` 等元信息一起存入带 `vector` 字段的表中；
- 检索时使用：
  - `embedding <-> query_vector` 做语义相似度排序；
  - `project_id = 当前项目`、`chapter_from <= 当前章节 <= chapter_to` 等条件进行过滤。

这种“关系库 + 向量库同库结合”的方案，有几个好处：
- 部署简单：单一 PostgreSQL 实例同时承担结构化与向量检索；
- 检索逻辑统一：复杂过滤条件（章节范围、实体类型、是否当前版本）可以直接用 SQL 表达；
- 数据一致性好：不需要在多个异构存储之间做复杂同步。

#### 2.4.3 为什么仍然需要 RAG，而不是“全量塞进上下文”？

如 `docs/RAG策略.md` 详细说明：
- 长篇小说的设定和正文量级远超单次上下文容量；
- 设定与正文随时在变动，完全依赖系统 Prompt 或 Finetune 不现实；
- 更合理的工程方案是：
  - 将项目设定持久化为**外置记忆库**；
  - 在每次生成前，由系统根据当前章节/场景/角色自动构造检索 Query，从 RAG 层拉取“此刻必须记住的那一小部分”；
  - 必要时，生成后再依据设定库做“对比检查”。

因此，本项目中 RAG 不仅用于“问答检索”，更是**写作引擎的前置上下文加载与一致性检查工具**。

### 2.5 LLM 接入与工具调用

- LLM 调用通过 `back-end/app/services/llm_service.py` 抽象，对外提供统一的 `chat`/`generate` 接口；
- 在 Agent 节点内部（`nodes.py`），通过调用该服务并结合 Prompt 模板（存放于 `back-end/prompts/`）实现专业化生成：
  - `prompts/genesis/*.txt` 用于骨架、主角、世界观、大纲、第一章等模块化生成；
- 项目支持：
  - 指定不同的基础模型（如 GPT-4o-mini 或兼容 DeepSeek 等）；
  - 通过 MCP 协议扩展外部工具能力（如 Web 搜索、知识库查询等）。

### 2.6 MCP 服务架构：外部工具的标准化封装

#### 2.6.1 什么是 MCP（Model Context Protocol）

MCP（Model Context Protocol）是一套标准化的协议，用于将外部工具、数据源和服务以统一的接口暴露给 AI 应用。本项目采用 MCP 架构将外部能力（如 Web 搜索）与核心业务逻辑解耦。

#### 2.6.2 Tavily 搜索的 MCP 封装

项目中的 Web 搜索功能通过 MCP 服务实现：

- **MCP 服务端**（`back-end/mcp_server/tavily_server.py`）：
  - 使用 FastMCP 框架创建独立的 MCP 服务；
  - 定义 `search_inspiration` 工具，封装 Tavily API 调用逻辑；
  - 通过 stdio 协议与主后端通信；
  - 支持高级搜索（`search_depth="advanced"`）以获取更丰富的结果。

- **MCP 客户端**（`back-end/app/services/inspiration_service.py`）：
  - 通过 `mcp.client.stdio` 连接到 MCP 服务端；
  - 将 Tavily API Key 等环境变量传递给子进程；
  - 实现重试机制（2 次重试 + 1 秒间隔）提高可靠性；
  - 优雅降级：搜索失败时返回友好错误并继续使用内置知识。

#### 2.6.3 为什么使用 MCP 而不是直接调用 Tavily SDK？

**架构解耦优势**：
- **可替换性**：切换搜索引擎（如从 Tavily 到 DuckDuckGo）只需替换 MCP 服务端，客户端代码无需修改；
- **多应用复用**：同一个 MCP 服务可被多个 AI 应用（如本项目未来可能的多租户版本）共享；
- **权限隔离**：API Key 仅在 MCP 服务进程中管理，主后端无需直接持有；
- **资源管理**：MCP 服务可独立扩展、监控和限流，不影响主服务性能。

**工程实践优势**：
- **标准化接口**：所有外部工具都通过统一的 MCP Tool 接口暴露，便于 Agent 调用；
- **易于测试**：可以单独测试 MCP 服务端，无需启动完整后端；
- **扩展性强**：未来可轻松添加更多 MCP 工具（如知识库查询、图片生成、代码执行等）。

#### 2.6.4 MCP 服务在 Genesis Wizard 中的应用

在创世向导（Genesis Wizard）流程中，MCP 搜索服务被集成到多个生成环节：

- **骨架生成（Skeleton）**：搜索流派典型元素和故事结构参考；
- **角色生成（Characters）**：搜索人名灵感、性格原型案例；
- **世界观生成（World）**：搜索力量体系、设定范例；
- **统一模式（Unified）**：根据用户输入智能触发搜索，补充背景知识。

前端通过 Toast 通知向用户展示搜索状态（成功/失败/降级），提供透明的生成过程反馈。

---

## 三、项目启动方法

以下内容基于 `docs/readme.md`，并结合当前仓库结构整理为更简洁的启动流程。

### 3.1 环境准备

请确保本地已安装：
- Python 3.10+（推荐 3.11）；
- Node.js 18+ 和 npm；
- Docker 与 Docker Compose（用于运行 PostgreSQL + pgvector）；
- 可选：pgAdmin，用于可视化查看和管理数据库。

### 3.2 启动后端服务

1. **启动数据库容器**

```bash
cd back-end
docker-compose up -d
```

2. **创建并激活 Python 虚拟环境**

```bash
cd back-end
python3 -m venv venv
source venv/bin/activate  # macOS / Linux
# Windows: venv\Scripts\activate
```

3. **安装依赖**

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. **配置环境变量**

```bash
cp .env.example .env
# 然后用编辑器修改 .env
```

需要至少配置：
- 数据库连接信息（`POSTGRES_*`、`DATABASE_URL`）；
- LLM 接口密钥（如 `OPENAI_API_KEY`、`LLM_BASE_URL`）；
- 向量/Embedding 接口密钥（`EMBEDDING_API_KEY` 等）；
- Web 搜索 API 密钥（`TAVILY_API_KEY`）。

5. **初始化数据库结构**

```bash
python -m app.db.init_db
```

6. **启动 FastAPI 服务**

在项目根目录或 `back-end` 目录下执行（推荐在 `back-end` 下）：

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

启动成功后：
- API 根地址：http://localhost:8000
- Swagger 文档：http://localhost:8000/docs

### 3.3 启动前端应用

1. **安装依赖**

```bash
cd front-end
npm install
```

2. **配置环境变量（如需要）**

若前端需要自定义后端 API 地址，可在 `.env` 或 `api.ts` 中调整：
- 默认通常使用 `http://localhost:8000`。

3. **启动开发服务器**

```bash
npm run dev
```

终端输出类似：

```bash
VITE vX.X.X  ready in XXX ms
  ➜  Local:   http://localhost:5173/
```

此时在浏览器访问：http://localhost:5173 即可使用应用。

### 3.4 常见问题简要提示

- 若后端启动报错数据库连接失败，优先检查 Docker 容器是否已启动，以及 `.env` 中 `DATABASE_URL` 是否配置正确；
- 若前端调用 API 失败，确认后端服务端口（默认为 8000）与 `front-end/api.ts` 中的配置是否一致；
- 若 LLM 或搜索相关功能报认证错误，请确认 `.env` 中的 API Key 已正确填写且未包含多余空格。

---

## 四、项目亮点与可扩展方向

本项目除了一个可用的长篇写作工具原型，更希望体现的是**整体架构设计与技术选型能力**。这里总结几个我认为有代表性的点：

### 4.1 以“项目记忆”为核心的 RAG 设计

- 将世界观、人物卡、时间线、章节摘要等拆分为多个知识子库，并通过统一的版本化 Schema 管理：
  - 通过 `chapter_from` / `chapter_to` 控制设定在故事时间上的有效范围；
  - 通过 `version` 与 `is_current` 支持设定随创作演化而升级；
- 每次生成章节时，自动按“当前项目 + 当前章节 + 场景/角色标签”从向量库检索对应设定，确保模型只看到当下应当生效的世界状态；
- 这种设计兼顾了：
  - **记忆完整性**：所有历史设定都被保留，可用于回溯与对比；
  - **上下文可控性**：单次调用只加载必要的设定片段，避免上下文爆炸；
  - **一致性检查**：可以在生成后再次用设定库对比文本，发现“吃书”问题。

### 4.2 LangGraph 多 Agent 协作与可视化进度

- 使用 LangGraph 将「骨架 → 角色 → 世界观 → 大纲 → 第一章」拆成一系列专业 Agent，使每个 Agent 都有相对稳定和清晰的职责；
- 通过 FastAPI 的 `StreamingResponse` 将各节点的状态（如 `status`、`result` 事件）实时推送至前端，前端据此更新 Genesis Wizard 中的进度条与结果展示；
- 这种设计相比“一次性大 Prompt”更易于：
  - 观察与调试中间步骤；
  - 对某个环节单独优化 Prompt 或模型；
  - 将来扩展“反思/重写/多版本候选”等高级能力。

### 4.3 工程实践与未来扩展

从工程角度，本项目还具备以下可扩展性：

- **模型与 Provider 解耦**：通过统一的 LLM Service 抽象，后续可以轻松接入不同模型（如 DeepSeek、Anthropic、Google Gemini 等）；
- **多项目/多用户支持**：以 `project_id` 为关键维度设计数据表和向量索引，为将来支持多用户、多作品场景打下基础；
- **功能扩展方向**：
  - 新增“设定冲突检测面板”：高亮展示正文与设定的差异点；
  - 引入更强的富文本编辑器（如 Tiptap），支持批注与多版本对比；
  - 引入作家工作流统计（更新频率、日更字数、剧情节奏分析等）。

---

## 五、如何在 Portfolio 中呈现本项目

如果将本项目放入个人作品集，可以重点强调以下几点：

- **从业务痛点出发的系统设计**：不是简单调用大模型，而是围绕“长篇创作的一致性与可演化性”设计数据结构与流程；
- **前后端与 AI 编排协同**：
  - 前端提供高度结构化和可视化的创作体验（Genesis Wizard / Studio / Wiki）；
  - 后端使用 FastAPI + LangGraph 编排多 Agent，使生成过程可控、可观察；
  - RAG 层用 PostgreSQL + pgvector 将“小说项目记忆”工程化落地；
- **良好的工程边界与可扩展性**：清晰的模块划分（Router、Service、DB、Agents），以及对未来模型替换和功能升级的预留空间。

通过这份项目，你可以在面试或 Portfolio 中完整展示：
- 对 LLM / Agent / RAG 的理解不止停留在概念层，而是落到具体 Schema 设计与调用链路；
- 能够把产品需求文档（`需求说明书.md`）、技术选型文档（`RAG策略.md`）与实际代码实现统一起来；
- 具备从 0 设计一套“有业务场景、有技术深度”的 AI 应用的能力。
