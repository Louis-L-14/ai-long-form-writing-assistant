from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Dict, Any, Optional
from app.services.llm_service import llm_service
import json

router = APIRouter()

class GenerateConceptRequest(BaseModel):
    user_input: str
    inspiration_context: Optional[str] = ""

class GenerateProtagonistRequest(BaseModel):
    user_input: str
    current_data: Dict[str, Any]

class GenerateWorldRequest(BaseModel):
    user_input: str
    current_data: Dict[str, Any]

class GenerateOutlineRequest(BaseModel):
    user_input: str
    current_data: Dict[str, Any]

class GenerateUnifiedRequest(BaseModel):
    user_input: str
    current_data: Dict[str, Any]
    inspiration_context: Optional[str] = ""

class GenerateFirstChapterRequest(BaseModel):
    user_input: str
    current_data: Dict[str, Any]

@router.post("/concept")
async def generate_concept(request: GenerateConceptRequest):
    """Generate core concept (title, genre, theme) for a new project."""
    
    # Load system prompt from file
    system_prompt = llm_service.load_prompt("genesis/concept.txt")
    
    # Construct user prompt
    user_prompt = f"""å½“å‰ä»»åŠ¡:ç¡®ç«‹æ ¸å¿ƒæ¦‚å¿µ
ç”¨æˆ·è¾“å…¥:"{request.user_input}"
å‚è€ƒçµæ„Ÿ:{request.inspiration_context}

è¯·åŸºäºç”¨æˆ·è¾“å…¥,ç”Ÿæˆä»¥ä¸‹ JSON æ•°æ®:
1. "title": è®¾è®¡ä¸€ä¸ªå¸å¼•äººçš„ä¸­æ–‡ä¹¦åã€‚
2. "genre": å®šä¹‰å…·ä½“çš„æµæ´¾(å¦‚:èµ›åšä¿®ä»™ã€å…‹è‹é²æ­¦ä¾ )ã€‚
3. "theme": æç‚¼æ ¸å¿ƒä¸»é¢˜æˆ–å†²çªã€‚
4. "response": ä»¥æ¶æ„å¸ˆçš„å£å»ç‚¹è¯„è¿™ä¸ªç‚¹å­,å¹¶å¼•å¯¼ç”¨æˆ·è¿›å…¥ä¸‹ä¸€æ­¥(ä¸»è§’è®¾å®š)ã€‚å¦‚æœç”¨æˆ·è¾“å…¥æ¨¡ç³Š,è¯·ä¸»åŠ¨æå‡ºå»ºè®®ã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return StreamingResponse(
        llm_service.generate_text(messages, stream=True),
        media_type="text/event-stream"
    )

class GenerateSkeletonRequest(BaseModel):
    user_input: str
    inspiration_context: Optional[str] = ""

@router.post("/skeleton")
async def generate_skeleton(request: GenerateSkeletonRequest):
    """Generate project skeleton (formula, goal, rules)."""
    
    system_prompt = llm_service.load_prompt("genesis/skeleton.txt")
    
    user_prompt = f"""å½“å‰ä»»åŠ¡:æ„å»ºéª¨æ¶å°çº²
ç”¨æˆ·è¾“å…¥:"{request.user_input}"
å‚è€ƒçµæ„Ÿ:{request.inspiration_context}

è¯·ç”Ÿæˆ JSON æ ¼å¼çš„éª¨æ¶å°çº²ï¼ŒåŒ…å«ï¼š
1. "title": æå…·å¸å¼•åŠ›çš„ä¹¦åï¼ˆé¡¹ç›®åç§°ï¼‰
2. "story_formula": æ•…äº‹å…¬å¼
3. "volume1_goal": ç¬¬ä¸€å·ç›®æ ‡
4. "golden_finger_rules": é‡‘æ‰‹æŒ‡è§„åˆ™ï¼ˆæ•°ç»„ï¼‰
5. "core_hook": æ ¸å¿ƒé’©å­
6. "emotional_tone": æƒ…æ„ŸåŸºè°ƒ"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return StreamingResponse(
        llm_service.generate_text(messages, stream=True),
        media_type="text/event-stream"
    )

@router.post("/protagonist")
async def generate_protagonist(request: GenerateProtagonistRequest):
    """Generate protagonist details based on established concept."""
    
    system_prompt = llm_service.load_prompt("genesis/protagonist.txt")
    
    user_prompt = f"""å½“å‰ä»»åŠ¡:å¡‘é€ è§’è‰²
å·²çŸ¥è“å›¾:{json.dumps(request.current_data, ensure_ascii=False)}
ç”¨æˆ·è¾“å…¥:"{request.user_input}"

è¯·å®Œå–„è§’è‰²è®¾å®š,ç”Ÿæˆ JSON:
1. "characters": è§’è‰²æ•°ç»„,æ¯ä¸ªè§’è‰²åŒ…å«: id, name, age, personality, appearance, background, relationships, role, goal, advantage
2. "response": æè¿°è§’è‰²çš„å½¢è±¡æˆ–æ ¸å¿ƒç‰¹è´¨,å¹¶å¼•å¯¼ç”¨æˆ·è¿›å…¥ä¸‹ä¸€æ­¥(ä¸–ç•Œè§‚è®¾å®š)ã€‚å¦‚æœç”¨æˆ·æœªæä¾›ç»†èŠ‚,è¯·æ ¹æ®æµæ´¾è‡ªåŠ¨ç”ŸæˆåŒ¹é…çš„è§’è‰²ã€‚

æ³¨æ„: role å¿…é¡»æ˜¯ "protagonist"(ä¸»è§’)ã€"antagonist"(åæ´¾) æˆ– "supporting"(é…è§’) ä¹‹ä¸€ã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return StreamingResponse(
        llm_service.generate_text(messages, stream=True),
        media_type="text/event-stream"
    )

@router.post("/world")
async def generate_world(request: GenerateWorldRequest):
    """Generate world-building elements (power system, factions)."""
    
    system_prompt = llm_service.load_prompt("genesis/world.txt")
    
    user_prompt = f"""å½“å‰ä»»åŠ¡:æ„å»ºä¸–ç•Œè§‚
å·²çŸ¥è“å›¾:{json.dumps(request.current_data, ensure_ascii=False)}
ç”¨æˆ·è¾“å…¥:"{request.user_input}"

è¯·å®Œå–„ä¸–ç•Œè§‚,ç”Ÿæˆ JSON:
1. "power_system": å®šä¹‰åŠ›é‡/å‡çº§ä½“ç³»ã€‚
2. "main_faction": è®¾è®¡ä¸»è¦çš„åæ´¾æˆ–å¯¹ç«‹åŠ¿åŠ›ã€‚
3. "world_details": åŒ…å« setting, core_rules, hidden_layers çš„å¯¹è±¡ã€‚
4. "response": æ€»ç»“ä¸–ç•Œè§‚è®¾å®š,å¹¶å¼•å¯¼ç”¨æˆ·è¿›å…¥ä¸‹ä¸€æ­¥(å¤§çº²ç”Ÿæˆ)ã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return StreamingResponse(
        llm_service.generate_text(messages, stream=True),
        media_type="text/event-stream"
    )

@router.post("/outline")
async def generate_outline(request: GenerateOutlineRequest):
    """Generate full story outline with volume-chapter structure."""
    
    system_prompt = llm_service.load_prompt("genesis/outline.txt")
    
    user_prompt = f"""å½“å‰ä»»åŠ¡:ç”Ÿæˆæ•…äº‹å¤§çº²
å·²çŸ¥è“å›¾:{json.dumps(request.current_data, ensure_ascii=False)}
ç”¨æˆ·è¾“å…¥:"{request.user_input}"

è¯·ç”Ÿæˆä¸€ä¸ªç¬¦åˆ"å·-ç« "ç»“æ„çš„æ•…äº‹å¤§çº²,è¿”å› JSON:
1. "outline": æ•°ç»„æ ¼å¼,æ¯ä¸ªå…ƒç´ åŒ…å«:
   - "id": "volume_N"
   - "title": "å·å"
   - "summary": "æœ¬å·æ¦‚è¦:é˜¶æ®µç›®æ ‡ã€ä¸»è¦å†²çªã€æƒ…ç»ªç»“æ„"
   - "world_unlock": "æœ¬å·è§£é”çš„ä¸–ç•Œè§‚ä¿¡æ¯"
   - "chapters": [{{"id": "chapter_N", "title": "ç« èŠ‚å", "summary": "ç« èŠ‚æ¦‚è¦", "plot_hook": "æ‚¬å¿µç‚¹"}}]
2. "foreshadowing": ä¼ç¬”æ¸…å•æ•°ç»„
3. "hidden_plot": æš—çº¿è§„åˆ’å¯¹è±¡
4. "response": ç®€è¿°è¿™ä¸ªå¤§çº²çš„æ•´ä½“ç»“æ„å’Œæ•…äº‹èµ°å‘,å¹¶çƒ­æƒ…åœ°é‚€è¯·ç”¨æˆ·å¼€å§‹æ­£æ–‡åˆ›ä½œã€‚

å»ºè®®ç”Ÿæˆ3-5å·,æ¯å·3-8ç« ã€‚æ³¨é‡å‰30ç« çš„çˆ½ç‚¹å¯†é›†åº¦å’Œæƒ…ç»ªæ³¢æµªè®¾è®¡ã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return StreamingResponse(
        llm_service.generate_text(messages, stream=True),
        media_type="text/event-stream"
    )

@router.post("/first_chapter")
async def generate_first_chapter(request: GenerateFirstChapterRequest):
    """Generate the first chapter based on all established project info."""
    
    system_prompt = llm_service.load_prompt("genesis/first_chapter.txt")
    
    user_prompt = f"""å½“å‰ä»»åŠ¡:æ’°å†™ç¬¬ä¸€ç« 
å·²çŸ¥è“å›¾:{json.dumps(request.current_data, ensure_ascii=False)}
ç”¨æˆ·è¾“å…¥:"{request.user_input}"

è¯·æ’°å†™ç¬¬ä¸€ç« æ­£æ–‡ï¼Œè¿”å› JSON:
1. "title": ç« èŠ‚æ ‡é¢˜
2. "content": ç« èŠ‚æ­£æ–‡ï¼ˆ2000å­—å·¦å³ï¼‰
3. "response": ç®€è¦è¯´æ˜æœ¬ç« çš„"é’©å­"è®¾è®¡æ€è·¯ã€‚

è¯·åŠ¡å¿…éµå¾ª"é»„é‡‘ä¸‰ç« "æ³•åˆ™ï¼Œå¼€ç¯‡å³é«˜æ½®ï¼Œç•™ä¸‹å¼ºæ‚¬å¿µã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return StreamingResponse(
        llm_service.generate_text(messages, stream=True),
        media_type="text/event-stream"
    )

class SuggestTitleRequest(BaseModel):
    project_data: Dict[str, Any]

def clean_json_string(text: str) -> str:
    """Clean LLM response to ensure valid JSON."""
    text = text.strip()
    # Remove markdown code blocks if present
    if text.startswith("```"):
        text = text.split("\n", 1)[1] if "\n" in text else text
        if text.endswith("```"):
            text = text.rsplit("\n", 1)[0]
    # Remove "json" language identifier if present
    if text.startswith("json"):
        text = text[4:].strip()
    return text

@router.post("/suggest_title")
async def suggest_title(request: SuggestTitleRequest):
    """Generate title suggestions using 3-step process: Analysis -> Search -> Synthesis."""
    from app.services.inspiration_service import inspiration_service
    
    project_data = request.project_data
    
    # Extract rich data
    genre = project_data.get("genre", "æœªå®š")
    theme = project_data.get("theme", "")
    description = project_data.get("description", "")
    story_formula = project_data.get("story_formula", "")
    core_hook = project_data.get("core_hook", "")
    
    # Handle complex objects (outline, characters, world) which might be lists or dicts
    outline = json.dumps(project_data.get("outline", []), ensure_ascii=False)[:2000] # Truncate to avoid context limit
    characters = json.dumps(project_data.get("characters", []), ensure_ascii=False)[:1500]
    world = json.dumps(project_data.get("world_details", {}), ensure_ascii=False)[:1000]
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ STARTING TITLE GENERATION (3-STEP PROCESS)")
    print(f"{'='*60}\n")

    # =================================================================================
    # STEP 1: DEEP ANALYSIS
    # =================================================================================
    print(f"--- STEP 1: DEEP ANALYSIS ---")
    analysis_prompt = llm_service.load_prompt("genesis/title_analysis_prompt.txt")
    analysis_user_prompt = f"""è¯·åˆ†æä»¥ä¸‹å°è¯´é¡¹ç›®ä¿¡æ¯ï¼š

ã€åŸºæœ¬ä¿¡æ¯ã€‘
ç±»å‹: {genre}
ä¸»é¢˜: {theme}
ä¸€å¥è¯ç®€ä»‹: {description}
æ•…äº‹å…¬å¼: {story_formula}
æ ¸å¿ƒé’©å­: {core_hook}

ã€å¤§çº²æ‘˜è¦ã€‘
{outline}

ã€ä¸»è¦è§’è‰²ã€‘
{characters}

ã€ä¸–ç•Œè§‚æ‘˜è¦ã€‘
{world}
"""
    
    analysis_messages = [
        {"role": "system", "content": analysis_prompt},
        {"role": "user", "content": analysis_user_prompt}
    ]
    
    analysis_response = ""
    async for chunk in llm_service.generate_text(analysis_messages, stream=False):
        analysis_response += chunk
        
    try:
        cleaned_response = clean_json_string(analysis_response)
        analysis_data = json.loads(cleaned_response)
        core_elements = analysis_data.get("core_elements", [])
        selling_point = analysis_data.get("selling_point_summary", "")
        naming_direction = analysis_data.get("naming_direction", "")
        
        print(f"Core Elements: {core_elements}")
        print(f"Selling Point: {selling_point}")
        print(f"Direction: {naming_direction}\n")
    except Exception as e:
        print(f"Analysis parsing failed: {e}")
        print(f"Raw response: {analysis_response}")
        core_elements = [genre, theme]
        selling_point = description
        naming_direction = "å¸¸è§„ç½‘æ–‡é£æ ¼"

    # =================================================================================
    # STEP 2: STYLE SEARCH
    # =================================================================================
    print(f"--- STEP 2: STYLE SEARCH ---")
    query_gen_prompt = llm_service.load_prompt("genesis/search_query_generator.txt")
    query_gen_user_prompt = f"""åŸºäºä»¥ä¸‹æ·±åº¦åˆ†æç»“æœç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼š

æ ¸å¿ƒå…ƒç´ : {json.dumps(core_elements, ensure_ascii=False)}
å–ç‚¹æ€»ç»“: {selling_point}
å‘½åæ–¹å‘: {naming_direction}
"""
    
    query_messages = [
        {"role": "system", "content": query_gen_prompt},
        {"role": "user", "content": query_gen_user_prompt}
    ]
    
    query_response = ""
    async for chunk in llm_service.generate_text(query_messages, stream=False):
        query_response += chunk
    
    try:
        cleaned_query = clean_json_string(query_response)
        query_data = json.loads(cleaned_query)
        search_queries = query_data.get("queries", [])
    except Exception as e:
        print(f"Query parsing failed: {e}")
        search_queries = [f"{genre} {core_elements[0]} çƒ­é—¨å°è¯´"]

    all_search_results = []
    for search_query in search_queries[:2]:
        print(f"ğŸ” Searching: {search_query}")
        try:
            search_results = await inspiration_service.search_inspiration(search_query)
            if "results" in search_results:
                # Log titles found
                found_titles = [r.get('title', 'N/A') for r in search_results["results"][:3]]
                print(f"   Found: {found_titles}")
                all_search_results.extend(search_results.get("results", [])[:5])
        except Exception as e:
            print(f"   Search failed: {e}")

    # =================================================================================
    # STEP 3: SYNTHESIS
    # =================================================================================
    print(f"\n--- STEP 3: SYNTHESIS ---")
    
    # Prepare context
    search_context = ""
    if all_search_results:
        search_context = "\nã€å¸‚åœºçƒ­é—¨å‚è€ƒã€‘\n"
        for idx, result in enumerate(all_search_results[:8], 1):
            title = result.get("title", "")
            content = result.get("content", "")[:100]
            search_context += f"{idx}. {title} ({content})\n"

    synthesis_system_prompt = """ä½ æ˜¯ä¸€ä½é‡‘ç‰Œç½‘æ–‡ç¼–è¾‘ã€‚è¯·æ ¹æ®é¡¹ç›®åˆ†æå’Œå¸‚åœºå‚è€ƒï¼Œåˆ›ä½œ5ä¸ªæå…·å¸å¼•åŠ›çš„ä¹¦åã€‚

è¦æ±‚ï¼š
1. **ç»“åˆæ ¸å¿ƒå…ƒç´ **ï¼šå¿…é¡»ä½“ç°åˆ†æå‡ºçš„æ ¸å¿ƒé‡‘æ‰‹æŒ‡ã€èº«ä»½æˆ–å†²çªã€‚
2. **å‚è€ƒå¸‚åœºé£å‘**ï¼šå€Ÿé‰´æœç´¢åˆ°çš„çƒ­é—¨ä¹¦åçš„å‘½åç»“æ„ï¼ˆå¦‚"XX+ç³»ç»Ÿ"ã€"å¼€å±€+XX"ï¼‰ï¼Œä½†ä¸¥ç¦æŠ„è¢­ã€‚
3. **é£æ ¼å¤šæ ·**ï¼šæä¾›ä¸åŒé£æ ¼çš„é€‰é¡¹ï¼ˆå¦‚ç›´ç™½çˆ½æ–‡é£ã€æ–‡è‰ºé£ã€æ‚¬ç–‘é£ï¼‰ã€‚
4. **è§£é‡Šç†ç”±**ï¼šä¸ºæ¯ä¸ªä¹¦åæä¾›ç®€çŸ­çš„æ¨èç†ç”±ã€‚

è¿”å› JSON:
{
  "suggestions": ["ä¹¦å1", "ä¹¦å2", "ä¹¦å3", "ä¹¦å4", "ä¹¦å5"]
}"""

    synthesis_user_prompt = f"""ã€é¡¹ç›®æ·±åº¦åˆ†æã€‘
æ ¸å¿ƒå…ƒç´ : {core_elements}
å–ç‚¹: {selling_point}
å»ºè®®æ–¹å‘: {naming_direction}

{search_context}

è¯·ç”Ÿæˆ5ä¸ªä¹¦åå»ºè®®ã€‚"""

    synthesis_messages = [
        {"role": "system", "content": synthesis_system_prompt},
        {"role": "user", "content": synthesis_user_prompt}
    ]

    suggestions_text = ""
    async for chunk in llm_service.generate_text(synthesis_messages, stream=False):
        suggestions_text += chunk
    
    try:
        cleaned_suggestions = clean_json_string(suggestions_text)
        suggestions_data = json.loads(cleaned_suggestions)
        print(f"âœ… Generated: {suggestions_data.get('suggestions', [])}")
        return suggestions_data
    except Exception as e:
        print(f"Synthesis parsing failed: {e}")
        print(f"Raw text: {suggestions_text}")
        return {"suggestions": ["æ•°æ®è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"]}

@router.post("/unified")
async def generate_unified(request: GenerateUnifiedRequest):
    """Unified AI entry using LangGraph multi-agent collaboration with event streaming."""
    from app.agents import genesis_graph, GenesisState
    
    # Prepare initial state
    initial_state: GenesisState = {
        "user_input": f"{request.user_input}\n\nå‚è€ƒçµæ„Ÿ:\n{request.inspiration_context}" if request.inspiration_context else request.user_input,
        "current_data": request.current_data,
        "target_agents": [],
        "agent_outputs": {},
        "final_response": ""
    }
    
    async def event_generator():
        aggregated_outputs: Dict[str, Any] = {}
        final_response_text = ""
        
        try:
            # Stream events from the graph
            async for event in genesis_graph.astream(initial_state):
                for node_name, state_update in event.items():
                    # Yield status update
                    yield json.dumps({"type": "status", "agent": node_name, "status": "working"}, ensure_ascii=False) + "\n"

                    # Track latest final response from finalizer
                    if "final_response" in state_update:
                        final_response_text = state_update["final_response"]
                    
                    # Yield intermediate results if available
                    current_outputs = state_update.get("agent_outputs", {})
                    if current_outputs:
                        # Merge into aggregated outputs so we don't lose earlier modules
                        aggregated_outputs = {**aggregated_outputs, **current_outputs}

                        partial_data = {}
                        # Skeleton fields
                        if "story_formula" in current_outputs:
                            partial_data["story_formula"] = current_outputs["story_formula"]
                        if "volume1_goal" in current_outputs:
                            partial_data["volume1_goal"] = current_outputs["volume1_goal"]
                        if "golden_finger_rules" in current_outputs:
                            partial_data["golden_finger_rules"] = current_outputs["golden_finger_rules"]
                        if "core_hook" in current_outputs:
                            partial_data["core_hook"] = current_outputs["core_hook"]
                        if "emotional_tone" in current_outputs:
                            partial_data["emotional_tone"] = current_outputs["emotional_tone"]

                        # Characters
                        if "characters" in current_outputs:
                            partial_data["characters"] = current_outputs["characters"]

                        # World
                        if "world" in current_outputs:
                            partial_data["world"] = current_outputs["world"]

                        # Outline
                        if "outline" in current_outputs:
                            partial_data["outline"] = current_outputs["outline"]

                        # First chapter (mapped to title/content for frontend compatibility)
                        if "first_chapter_title" in current_outputs or "first_chapter_content" in current_outputs:
                            partial_data["title"] = current_outputs.get("first_chapter_title", "")
                            partial_data["content"] = current_outputs.get("first_chapter_content", "")

                        # Concept (if router produced it)
                        if "concept" in current_outputs:
                            partial_data["concept"] = current_outputs["concept"]
                        
                        if partial_data:
                            yield json.dumps({"type": "result", "data": partial_data}, ensure_ascii=False) + "\n"
            
            # After all nodes complete, yield final result
            response_data = {"response": final_response_text}

            # Attach all aggregated agent outputs
            for key, value in aggregated_outputs.items():
                # first chapter is remapped for frontend compatibility
                if key in ("first_chapter_title", "first_chapter_content"):
                    continue
                response_data[key] = value

            if "first_chapter_title" in aggregated_outputs:
                response_data["title"] = aggregated_outputs["first_chapter_title"]
                response_data["content"] = aggregated_outputs.get("first_chapter_content", "")

            # Yield final result
            yield json.dumps({"type": "result", "data": response_data}, ensure_ascii=False) + "\n"
            
        except Exception as e:
            import traceback
            print(f"Error in event_generator: {traceback.format_exc()}")
            yield json.dumps({"type": "status", "message": f"ç³»ç»Ÿé”™è¯¯: {str(e)}"}, ensure_ascii=False) + "\n"

    return StreamingResponse(
        event_generator(),
        media_type="application/x-ndjson"
    )
